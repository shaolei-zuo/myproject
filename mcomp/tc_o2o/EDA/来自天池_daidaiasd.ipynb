{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.6.29281b48TXW0mG&postId=58107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# In[] 读入源数据\n",
    "def get_source_data():\n",
    "    # 源数据路径\n",
    "    DataPath = '../data'\n",
    "\n",
    "    # 读入源数据\n",
    "    off_train = pd.read_csv(os.path.join(DataPath, 'ccf_offline_stage1_train.csv'),\n",
    "                            parse_dates=['Date_received', 'Date'])\n",
    "    off_train.columns = ['User_id', 'Merchant_id', 'Coupon_id', 'Discount_rate', 'Distance', 'Date_received', 'Date']\n",
    "\n",
    "    on_train = pd.read_csv(os.path.join(DataPath, 'ccf_online_stage1_train.csv'), parse_dates=['Date_received', 'Date'])\n",
    "    on_train.columns = ['User_id', 'Merchant_id', 'Action', 'Coupon_id', 'Discount_rate', 'Date_received', 'Date']\n",
    "\n",
    "    off_test = pd.read_csv(os.path.join(DataPath, 'ccf_offline_stage1_test_revised.csv'), parse_dates=['Date_received'])\n",
    "    off_test.columns = ['User_id', 'Merchant_id', 'Coupon_id', 'Discount_rate', 'Distance', 'Date_received']\n",
    "\n",
    "    print(off_train.info())\n",
    "    print(off_train.head(5))\n",
    "    return off_train, on_train, off_test\n",
    "\n",
    "\n",
    "# In[] null,na 特殊处理\n",
    "def null_process_offline(dataset, predict=False):\n",
    "    dataset.Distance.fillna(11, inplace=True)\n",
    "    dataset.Distance = dataset.Distance.astype(int)\n",
    "    dataset.Coupon_id.fillna(0, inplace=True)\n",
    "    dataset.Coupon_id = dataset.Coupon_id.astype(int)\n",
    "    dataset.Date_received.fillna(date_null, inplace=True)\n",
    "\n",
    "    dataset[['discount_rate_x', 'discount_rate_y']] = dataset[dataset.Discount_rate.str.contains(':') == True][\n",
    "        'Discount_rate'].str.split(':', expand=True).astype(int)\n",
    "    dataset['discount_rate'] = 1 - dataset.discount_rate_y / dataset.discount_rate_x\n",
    "    dataset.discount_rate = dataset.discount_rate.fillna(dataset.Discount_rate).astype(float)\n",
    "    if predict:\n",
    "        return dataset\n",
    "    else:\n",
    "        dataset.Date.fillna(date_null, inplace=True)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "def null_process_online(dataset):\n",
    "    dataset.Coupon_id.fillna(0, inplace=True)\n",
    "    # online.Coupon_id = online.Coupon_id.astype(int)\n",
    "    dataset.Date_received.fillna(date_null, inplace=True)\n",
    "    dataset.Date.fillna(date_null, inplace=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# In[] 生成交叉训练集\n",
    "def data_process(off_train, on_train, off_test):\n",
    "    # train feature split\n",
    "    # 交叉训练集一：收到券的日期大于4月14日和小于5月14日\n",
    "    time_range = ['2016-04-16', '2016-05-15']\n",
    "    dataset1 = off_train[(off_train.Date_received >= time_range[0]) & (off_train.Date_received <= time_range[1])].copy()\n",
    "    dataset1['label'] = 0\n",
    "    dataset1.loc[\n",
    "        (dataset1.Date != date_null) & (dataset1.Date - dataset1.Date_received <= datetime.timedelta(15)), 'label'] = 1\n",
    "    # 交叉训练集一特征offline：线下数据中领券和用券日期大于1月1日和小于4月13日\n",
    "    time_range_date_received = ['2016-01-01', '2016-03-31']\n",
    "    time_range_date = ['2016-01-01', '2016-04-15']\n",
    "    feature1_off = off_train[(off_train.Date >= time_range_date[0]) & (off_train.Date <= time_range_date[1]) | (\n",
    "            (off_train.Coupon_id == 0) & (off_train.Date_received >= time_range_date_received[0]) & (\n",
    "            off_train.Date_received <= time_range_date_received[1]))]\n",
    "    # 交叉训练集一特征online：线上数据中领券和用券日期大于1月1日和小于4月13日[on_train.date == 'null' to on_train.coupon_id == 0]\n",
    "    feature1_on = on_train[(on_train.Date >= time_range_date[0]) & (on_train.Date <= time_range_date[1]) | (\n",
    "            (on_train.Coupon_id == 0) & (on_train.Date_received >= time_range_date_received[0]) & (\n",
    "            on_train.Date_received <= time_range_date_received[1]))]\n",
    "\n",
    "    # 交叉训练集二：收到券的日期大于5月15日和小于6月15日\n",
    "    time_range = ['2016-05-16', '2016-06-15']\n",
    "    dataset2 = off_train[(off_train.Date_received >= time_range[0]) & (off_train.Date_received <= time_range[1])]\n",
    "    dataset2['label'] = 0\n",
    "    dataset2.loc[\n",
    "        (dataset2.Date != date_null) & (dataset2.Date - dataset2.Date_received <= datetime.timedelta(15)), 'label'] = 1\n",
    "    # 交叉训练集二特征offline：线下数据中领券和用券日期大于2月1日和小于5月14日\n",
    "    time_range_date_received = ['2016-02-01', '2016-04-30']\n",
    "    time_range_date = ['2016-02-01', '2016-05-15']\n",
    "    feature2_off = off_train[(off_train.Date >= time_range_date[0]) & (off_train.Date <= time_range_date[1]) | (\n",
    "            (off_train.Coupon_id == 0) & (off_train.Date_received >= time_range_date_received[0]) & (\n",
    "            off_train.Date_received <= time_range_date_received[1]))]\n",
    "    # 交叉训练集二特征online：线上数据中领券和用券日期大于2月1日和小于5月14日\n",
    "    feature2_on = on_train[(on_train.Date >= time_range_date[0]) & (on_train.Date <= time_range_date[1]) | (\n",
    "            (on_train.Coupon_id == 0) & (on_train.Date_received >= time_range_date_received[0]) & (\n",
    "            on_train.Date_received <= time_range_date_received[1]))]\n",
    "\n",
    "    # 测试集\n",
    "    dataset3 = off_test\n",
    "    # 测试集特征offline :线下数据中领券和用券日期大于3月15日和小于6月30日的\n",
    "    time_range = ['2016-03-16', '2016-06-30']\n",
    "    feature3_off = off_train[((off_train.Date >= time_range[0]) & (off_train.Date <= time_range[1])) | (\n",
    "            (off_train.Coupon_id == 0) & (off_train.Date_received >= time_range[0]) & (\n",
    "            off_train.Date_received <= time_range[1]))]\n",
    "    # 测试集特征online :线上数据中领券和用券日期大于3月15日和小于6月30日的\n",
    "    feature3_on = on_train[((on_train.Date >= time_range[0]) & (on_train.Date <= time_range[1])) | (\n",
    "            (on_train.Coupon_id == 0) & (on_train.Date_received >= time_range[0]) & (\n",
    "            on_train.Date_received <= time_range[1]))]\n",
    "\n",
    "    # get train feature\n",
    "    ProcessDataSet1 = get_features(dataset1, feature1_off, feature1_on)\n",
    "    ProcessDataSet2 = get_features(dataset2, feature2_off, feature2_on)\n",
    "    ProcessDataSet3 = get_features(dataset3, feature3_off, feature3_on)\n",
    "\n",
    "    return ProcessDataSet1, ProcessDataSet2, ProcessDataSet3\n",
    "\n",
    "\n",
    "def get_features(dataset, feature_off, feature_on):\n",
    "    dataset = get_offline_features(dataset, feature_off)\n",
    "    return get_online_features(feature_on, dataset)\n",
    "\n",
    "\n",
    "# In[] 定义获取feature的函数\n",
    "def get_offline_features(X, offline):\n",
    "    # X = X[:1000]\n",
    "\n",
    "    print(len(X), len(X.columns))\n",
    "\n",
    "    temp = offline[offline.Coupon_id != 0]\n",
    "    coupon_consume = temp[temp.Date != date_null]\n",
    "    coupon_no_consume = temp[temp.Date == date_null]\n",
    "\n",
    "    user_coupon_consume = coupon_consume.groupby('User_id')\n",
    "\n",
    "    X['weekday'] = X.Date_received.dt.weekday\n",
    "    X['day'] = X.Date_received.dt.day\n",
    "\n",
    "    # # 距离优惠券消费次数\n",
    "    # temp = coupon_consume.groupby('Distance').size().reset_index(name='distance_0')\n",
    "    # X = pd.merge(X, temp, how='left', on='Distance')\n",
    "    #\n",
    "    # # 距离优惠券不消费次数\n",
    "    # temp = coupon_no_consume.groupby('Distance').size().reset_index(name='distance_1')\n",
    "    # X = pd.merge(X, temp, how='left', on='Distance')\n",
    "    #\n",
    "    # # 距离优惠券领取次数\n",
    "    # X['distance_2'] = X.distance_0 + X.distance_1\n",
    "    #\n",
    "    # # 距离优惠券消费率\n",
    "    # X['distance_3'] = X.distance_0 / X.distance_2\n",
    "\n",
    "    # temp = coupon_consume[coupon_consume.Distance != 11].groupby('Distance').size()\n",
    "    # temp['d4'] = temp.Distance.sum() / len(temp)\n",
    "    # X = pd.merge(X, temp, how='left', on='Distance')\n",
    "\n",
    "    '''user features'''\n",
    "\n",
    "    # 优惠券消费次数\n",
    "    temp = user_coupon_consume.size().reset_index(name='u2')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "    # X.u2.fillna(0, inplace=True)\n",
    "    # X.u2 = X.u2.astype(int)\n",
    "\n",
    "    # 优惠券不消费次数\n",
    "    temp = coupon_no_consume.groupby('User_id').size().reset_index(name='u3')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 使用优惠券次数与没使用优惠券次数比值\n",
    "    X['u19'] = X.u2 / X.u3\n",
    "\n",
    "    # 领取优惠券次数\n",
    "    X['u1'] = X.u2.fillna(0) + X.u3.fillna(0)\n",
    "\n",
    "    # 优惠券核销率\n",
    "    X['u4'] = X.u2 / X.u1\n",
    "\n",
    "    # 普通消费次数\n",
    "    temp = offline[(offline.Coupon_id == 0) & (offline.Date != date_null)]\n",
    "    temp1 = temp.groupby('User_id').size().reset_index(name='u5')\n",
    "    X = pd.merge(X, temp1, how='left', on='User_id')\n",
    "\n",
    "    # 一共消费多少次\n",
    "    X['u25'] = X.u2 + X.u5\n",
    "\n",
    "    # 用户使用优惠券消费占比\n",
    "    X['u20'] = X.u2 / X.u25\n",
    "\n",
    "    # 正常消费平均间隔\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').size().reset_index(name='len'))\n",
    "    temp['u6'] = ((temp['max'] - temp['min']).dt.days / (temp['len'] - 1))\n",
    "    temp = temp.drop_duplicates('User_id')\n",
    "    X = pd.merge(X, temp[['User_id', 'u6']], how='left', on='User_id')\n",
    "\n",
    "    # 优惠券消费平均间隔\n",
    "    temp = pd.merge(coupon_consume, user_coupon_consume.Date.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').size().reset_index(name='len'))\n",
    "    temp['u7'] = ((temp['max'] - temp['min']).dt.days / (temp['len'] - 1))\n",
    "    temp = temp.drop_duplicates('User_id')\n",
    "    X = pd.merge(X, temp[['User_id', 'u7']], how='left', on='User_id')\n",
    "\n",
    "    # 15天内平均会普通消费几次\n",
    "    X['u8'] = X.u6 / 15\n",
    "\n",
    "    # 15天内平均会优惠券消费几次\n",
    "    X['u9'] = X.u7 / 15\n",
    "\n",
    "    # 领取优惠券到使用优惠券的平均间隔时间\n",
    "    temp = coupon_consume.copy()\n",
    "    temp['days'] = (temp.Date - temp.Date_received).dt.days\n",
    "    temp = (temp.groupby('User_id').days.sum() / temp.groupby('User_id').size()).reset_index(name='u10')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 在15天内使用掉优惠券的值大小\n",
    "    X['u11'] = X.u10 / 15\n",
    "\n",
    "    # 领取优惠券到使用优惠券间隔小于15天的次数\n",
    "    temp = coupon_consume.copy()\n",
    "    temp['days'] = (temp.Date - temp.Date_received).dt.days\n",
    "    temp = temp[temp.days <= 15]\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u21')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户15天使用掉优惠券的次数除以使用优惠券的次数\n",
    "    X['u22'] = X.u21 / X.u2\n",
    "\n",
    "    # 用户15天使用掉优惠券的次数除以领取优惠券未消费的次数\n",
    "    X['u23'] = X.u21 / X.u3\n",
    "\n",
    "    # 用户15天使用掉优惠券的次数除以领取优惠券的总次数\n",
    "    X['u24'] = X.u21 / X.u1\n",
    "\n",
    "    # 消费优惠券的平均折率\n",
    "    temp = user_coupon_consume.discount_rate.mean().reset_index(name='u45')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券的最低消费折率\n",
    "    temp = user_coupon_consume.discount_rate.min().reset_index(name='u27')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券的最高消费折率\n",
    "    temp = user_coupon_consume.discount_rate.max().reset_index(name='u28')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销过的不同优惠券数量\n",
    "    temp = coupon_consume.groupby(['User_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u32')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户领取所有不同优惠券数量\n",
    "    temp = offline[offline.Date_received != date_null]\n",
    "    temp = temp.groupby(['User_id', 'Coupon_id']).size().reset_index(name='u47')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Coupon_id'])\n",
    "\n",
    "    # 用户核销过的不同优惠券数量占所有不同优惠券的比重\n",
    "    X['u33'] = X.u32 / X.u47\n",
    "\n",
    "    # 用户平均每种优惠券核销多少张\n",
    "    X['u34'] = X.u2 / X.u47\n",
    "\n",
    "    # 核销优惠券用户-商家平均距离\n",
    "    temp = offline[(offline.Coupon_id != 0) & (offline.Date != date_null) & (offline.Distance != 11)]\n",
    "    temp = temp.groupby('User_id').Distance\n",
    "    temp = pd.merge(temp.count().reset_index(name='x'), temp.sum().reset_index(name='y'), on='User_id')\n",
    "    temp['u35'] = temp.y / temp.x\n",
    "    temp = temp[['User_id', 'u35']]\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券中的最小用户-商家距离\n",
    "    temp = coupon_consume[coupon_consume.Distance != 11]\n",
    "    temp = temp.groupby('User_id').Distance.min().reset_index(name='u36')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券中的最大用户-商家距离\n",
    "    temp = coupon_consume[coupon_consume.Distance != 11]\n",
    "    temp = temp.groupby('User_id').Distance.max().reset_index(name='u37')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 优惠券类型\n",
    "    discount_types = [\n",
    "        '0.2', '0.5', '0.6', '0.7', '0.75', '0.8', '0.85', '0.9', '0.95', '30:20', '50:30', '10:5',\n",
    "        '20:10', '100:50', '200:100', '50:20', '30:10', '150:50', '100:30', '20:5', '200:50', '5:1',\n",
    "        '50:10', '100:20', '150:30', '30:5', '300:50', '200:30', '150:20', '10:1', '50:5', '100:10',\n",
    "        '200:20', '300:30', '150:10', '300:20', '500:30', '20:1', '100:5', '200:10', '30:1', '150:5',\n",
    "        '300:10', '200:5', '50:1', '100:1',\n",
    "    ]\n",
    "    X['discount_type'] = -1\n",
    "    for k, v in enumerate(discount_types):\n",
    "        X.loc[X.Discount_rate == v, 'discount_type'] = k\n",
    "\n",
    "    # 不同优惠券领取次数\n",
    "    temp = offline.groupby(['User_id', 'Discount_rate']).size().reset_index(name='u41')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Discount_rate'])\n",
    "\n",
    "    # 不同优惠券使用次数\n",
    "    temp = coupon_consume.groupby(['User_id', 'Discount_rate']).size().reset_index(name='u42')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Discount_rate'])\n",
    "\n",
    "    # 不同优惠券不使用次数\n",
    "    temp = coupon_no_consume.groupby(['User_id', 'Discount_rate']).size().reset_index(name='u43')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Discount_rate'])\n",
    "\n",
    "    # 不同打折优惠券使用率\n",
    "    X['u44'] = X.u42 / X.u41\n",
    "\n",
    "    # 满减类型优惠券领取次数\n",
    "    temp = offline[offline.Discount_rate.str.contains(':') == True]\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u48')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 打折类型优惠券领取次数\n",
    "    temp = offline[offline.Discount_rate.str.contains('\\.') == True]\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='u49')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    '''offline merchant features'''\n",
    "\n",
    "    # 商户消费次数\n",
    "    temp = offline[offline.Date != date_null].groupby('Merchant_id').size().reset_index(name='m0')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券被领取后核销次数\n",
    "    temp = coupon_consume.groupby('Merchant_id').size().reset_index(name='m1')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商户正常消费笔数\n",
    "    X['m2'] = X.m0.fillna(0) - X.m1.fillna(0)\n",
    "\n",
    "    # 商家优惠券被领取次数\n",
    "    temp = offline[offline.Date_received != date_null].groupby('Merchant_id').size().reset_index(name='m3')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券被领取后核销率\n",
    "    X['m4'] = X.m1 / X.m3\n",
    "\n",
    "    # 商家优惠券被领取后不核销次数\n",
    "    temp = coupon_no_consume.groupby('Merchant_id').size().reset_index(name='m7')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商户当天优惠券领取次数\n",
    "    temp = X[X.Date_received != date_null]\n",
    "    temp = temp.groupby(['Merchant_id', 'Date_received']).size().reset_index(name='m5')\n",
    "    X = pd.merge(X, temp, how='left', on=['Merchant_id', 'Date_received'])\n",
    "\n",
    "    # 商户当天优惠券领取人数\n",
    "    temp = X[X.Date_received != date_null]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id', 'Date_received']).size().reset_index()\n",
    "    temp = temp.groupby(['Merchant_id', 'Date_received']).size().reset_index(name='m6')\n",
    "    X = pd.merge(X, temp, how='left', on=['Merchant_id', 'Date_received'])\n",
    "\n",
    "    # 商家优惠券核销的平均消费折率\n",
    "    temp = coupon_consume.groupby('Merchant_id').discount_rate.mean().reset_index(name='m8')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券核销的最小消费折率\n",
    "    temp = coupon_consume.groupby('Merchant_id').discount_rate.max().reset_index(name='m9')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券核销的最大消费折率\n",
    "    temp = coupon_consume.groupby('Merchant_id').discount_rate.min().reset_index(name='m10')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券核销不同的用户数量\n",
    "    temp = coupon_consume.groupby(['Merchant_id', 'User_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='m11')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家优惠券领取不同的用户数量\n",
    "    temp = offline[offline.Date_received != date_null].groupby(['Merchant_id', 'User_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='m12')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 核销商家优惠券的不同用户数量其占领取不同的用户比重\n",
    "    X['m13'] = X.m11 / X.m12\n",
    "\n",
    "    # 商家优惠券平均每个用户核销多少张\n",
    "    X['m14'] = X.m1 / X.m12\n",
    "\n",
    "    # 商家被核销过的不同优惠券数量\n",
    "    temp = coupon_consume.groupby(['Merchant_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='m15')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家领取过的不同优惠券数量的比重\n",
    "    temp = offline[offline.Date_received != date_null].groupby(['Merchant_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').count().reset_index(name='m18')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被核销过的不同优惠券数量占所有领取过的不同优惠券数量的比重\n",
    "    X['m19'] = X.m15 / X.m18\n",
    "\n",
    "    # 商家被核销优惠券的平均时间\n",
    "    temp = pd.merge(coupon_consume, coupon_consume.groupby('Merchant_id').Date.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('Merchant_id').Date.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('Merchant_id').size().reset_index(name='len'))\n",
    "    temp['m20'] = ((temp['max'] - temp['min']).dt.days / (temp['len'] - 1))\n",
    "    temp = temp.drop_duplicates('Merchant_id')\n",
    "    X = pd.merge(X, temp[['Merchant_id', 'm20']], how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被核销优惠券中的用户-商家平均距离\n",
    "    temp = coupon_consume[coupon_consume.Distance != 11].groupby('Merchant_id').Distance\n",
    "    temp = pd.merge(temp.count().reset_index(name='x'), temp.sum().reset_index(name='y'), on='Merchant_id')\n",
    "    temp['m21'] = temp.y / temp.x\n",
    "    temp = temp[['Merchant_id', 'm21']]\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被核销优惠券中的用户-商家最小距离\n",
    "    temp = coupon_consume[coupon_consume.Distance != 11]\n",
    "    temp = temp.groupby('Merchant_id').Distance.min().reset_index(name='m22')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被核销优惠券中的用户-商家最大距离\n",
    "    temp = coupon_consume[coupon_consume.Distance != 11]\n",
    "    temp = temp.groupby('Merchant_id').Distance.max().reset_index(name='m23')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    \"\"\"offline coupon features\"\"\"\n",
    "\n",
    "    # 此优惠券一共发行多少张\n",
    "    temp = offline[offline.Coupon_id != 0].groupby('Coupon_id').size().reset_index(name='c1')\n",
    "    X = pd.merge(X, temp, how='left', on='Coupon_id')\n",
    "\n",
    "    # 此优惠券一共被使用多少张\n",
    "    temp = coupon_consume.groupby('Coupon_id').size().reset_index(name='c2')\n",
    "    X = pd.merge(X, temp, how='left', on='Coupon_id')\n",
    "\n",
    "    # 优惠券使用率\n",
    "    X['c3'] = X.c2 / X.c1\n",
    "\n",
    "    # 没有使用的数目\n",
    "    X['c4'] = X.c1 - X.c2\n",
    "\n",
    "    # 此优惠券在当天发行了多少张\n",
    "    temp = X.groupby(['Coupon_id', 'Date_received']).size().reset_index(name='c5')\n",
    "    X = pd.merge(X, temp, how='left', on=['Coupon_id', 'Date_received'])\n",
    "\n",
    "    # 优惠券类型(直接优惠为0, 满减为1)\n",
    "    X['c6'] = 0\n",
    "    X.loc[X.Discount_rate.str.contains(':') == True, 'c6'] = 1\n",
    "\n",
    "    # 不同打折优惠券领取次数\n",
    "    temp = offline.groupby('Discount_rate').size().reset_index(name='c8')\n",
    "    X = pd.merge(X, temp, how='left', on='Discount_rate')\n",
    "\n",
    "    # 不同打折优惠券使用次数\n",
    "    temp = coupon_consume.groupby('Discount_rate').size().reset_index(name='c9')\n",
    "    X = pd.merge(X, temp, how='left', on='Discount_rate')\n",
    "\n",
    "    # 不同打折优惠券不使用次数\n",
    "    temp = coupon_no_consume.groupby('Discount_rate').size().reset_index(name='c10')\n",
    "    X = pd.merge(X, temp, how='left', on='Discount_rate')\n",
    "\n",
    "    # 不同打折优惠券使用率\n",
    "    X['c11'] = X.c9 / X.c8\n",
    "\n",
    "    # 优惠券核销平均时间\n",
    "    temp = pd.merge(coupon_consume, coupon_consume.groupby('Coupon_id').Date.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('Coupon_id').Date.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('Coupon_id').size().reset_index(name='count'))\n",
    "    temp['c12'] = ((temp['max'] - temp['min']).dt.days / (temp['count'] - 1))\n",
    "    temp = temp.drop_duplicates('Coupon_id')\n",
    "    X = pd.merge(X, temp[['Coupon_id', 'c12']], how='left', on='Coupon_id')\n",
    "\n",
    "    '''user merchant feature'''\n",
    "\n",
    "    # 用户领取商家的优惠券次数\n",
    "    temp = offline[offline.Coupon_id != 0]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id']).size().reset_index(name='um1')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户领取商家的优惠券后不核销次数\n",
    "    temp = coupon_no_consume.groupby(['User_id', 'Merchant_id']).size().reset_index(name='um2')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户领取商家的优惠券后核销次数\n",
    "    temp = coupon_consume.groupby(['User_id', 'Merchant_id']).size().reset_index(name='um3')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户领取商家的优惠券后核销率\n",
    "    X['um4'] = X.um3 / X.um1\n",
    "\n",
    "    # 用户对每个商家的不核销次数占用户总的不核销次数的比重\n",
    "    temp = coupon_no_consume.groupby('User_id').size().reset_index(name='temp')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "    X['um5'] = X.um2 / X.temp\n",
    "    X.drop(columns='temp', inplace=True)\n",
    "\n",
    "    # 用户在商店总共消费过几次\n",
    "    temp = offline[offline.Date != date_null].groupby(['User_id', 'Merchant_id']).size().reset_index(name='um6')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户在商店普通消费次数\n",
    "    temp = offline[(offline.Coupon_id == 0) & (offline.Date != date_null)]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id']).size().reset_index(name='um7')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户当天在此商店领取的优惠券数目\n",
    "    temp = offline[offline.Date_received != date_null]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id', 'Date_received']).size().reset_index(name='um8')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Merchant_id', 'Date_received'])\n",
    "\n",
    "    # 用户领取优惠券不同商家数量\n",
    "    temp = offline[offline.Coupon_id == offline.Coupon_id]\n",
    "    temp = temp.groupby(['User_id', 'Merchant_id']).size().reset_index()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='um9')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销优惠券不同商家数量\n",
    "    temp = coupon_consume.groupby(['User_id', 'Merchant_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='um10')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户核销过优惠券的不同商家数量占所有不同商家的比重\n",
    "    X['um11'] = X.um10 / X.um9\n",
    "\n",
    "    # 用户平均核销每个商家多少张优惠券\n",
    "    X['um12'] = X.u2 / X.um9\n",
    "\n",
    "    '''other feature'''\n",
    "\n",
    "    # 用户领取的所有优惠券数目\n",
    "    temp = X.groupby('User_id').size().reset_index(name='o1')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户领取的特定优惠券数目\n",
    "    temp = X.groupby(['User_id', 'Coupon_id']).size().reset_index(name='o2')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Coupon_id'])\n",
    "\n",
    "    # multiple threads\n",
    "    # data split\n",
    "    stop = len(X)\n",
    "    step = int(ceil(stop / cpu_jobs))\n",
    "\n",
    "    X_chunks = [X[i:i + step] for i in range(0, stop, step)]\n",
    "    X_list = [X] * cpu_jobs\n",
    "    counters = [i for i in range(cpu_jobs)]\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    #***##################################################################\n",
    "    #with ProcessPoolExecutor() as e:\n",
    "    #    X = pd.concat(e.map(task, X_chunks, X_list, counters))\n",
    "    #    print('time:', str(datetime.datetime.now() - start).split('.')[0])\n",
    "    X = pd.concat(map(task, X_chunks, X_list, counters))\n",
    "    # multiple threads\n",
    "    \n",
    "\n",
    "    # 用户领取优惠券平均时间间隔\n",
    "    temp = pd.merge(X, X.groupby('User_id').Date_received.max().reset_index(name='max'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').Date_received.min().reset_index(name='min'))\n",
    "    temp = pd.merge(temp, temp.groupby('User_id').size().reset_index(name='len'))\n",
    "    temp['o7'] = ((temp['max'] - temp['min']).dt.days / (temp['len'] - 1))\n",
    "    temp = temp.drop_duplicates('User_id')\n",
    "    X = pd.merge(X, temp[['User_id', 'o7']], how='left', on='User_id')\n",
    "\n",
    "    # 用户领取特定商家的优惠券数目\n",
    "    temp = X.groupby(['User_id', 'Merchant_id']).size().reset_index(name='o8')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Merchant_id'])\n",
    "\n",
    "    # 用户领取的不同商家数目\n",
    "    temp = X.groupby(['User_id', 'Merchant_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='o9')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户当天领取的优惠券数目\n",
    "    temp = X.groupby(['User_id', 'Date_received']).size().reset_index(name='o10')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Date_received'])\n",
    "\n",
    "    # 用户当天领取的特定优惠券数目\n",
    "    temp = X.groupby(['User_id', 'Coupon_id', 'Date_received']).size().reset_index(name='o11')\n",
    "    X = pd.merge(X, temp, how='left', on=['User_id', 'Coupon_id', 'Date_received'])\n",
    "\n",
    "    # 用户领取的所有优惠券种类数目\n",
    "    temp = X.groupby(['User_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='o12')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 商家被领取的优惠券数目\n",
    "    temp = X.groupby('Merchant_id').size().reset_index(name='o13')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家被领取的特定优惠券数目\n",
    "    temp = X.groupby(['Merchant_id', 'Coupon_id']).size().reset_index(name='o14')\n",
    "    X = pd.merge(X, temp, how='left', on=['Merchant_id', 'Coupon_id'])\n",
    "\n",
    "    # 商家被多少不同用户领取的数目\n",
    "    temp = X.groupby(['Merchant_id', 'User_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='o15')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    # 商家发行的所有优惠券种类数目\n",
    "    temp = X.groupby(['Merchant_id', 'Coupon_id']).size()\n",
    "    temp = temp.groupby('Merchant_id').size().reset_index(name='o16')\n",
    "    X = pd.merge(X, temp, how='left', on='Merchant_id')\n",
    "\n",
    "    print(len(X), len(X.columns))\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_online_features(online, X):\n",
    "    # temp = online[online.Coupon_id == online.Coupon_id]\n",
    "    # coupon_consume = temp[temp.Date == temp.Date]\n",
    "    # coupon_no_consume = temp[temp.Date != temp.Date]\n",
    "\n",
    "    # 用户线上操作次数\n",
    "    temp = online.groupby('User_id').size().reset_index(name='on_u1')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户线上点击次数\n",
    "    temp = online[online.Action == 0].groupby('User_id').size().reset_index(name='on_u2')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户线上点击率\n",
    "    X['on_u3'] = X.on_u2 / X.on_u1\n",
    "\n",
    "    # 用户线上购买次数\n",
    "    temp = online[online.Action == 1].groupby('User_id').size().reset_index(name='on_u4')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户线上购买率\n",
    "    X['on_u5'] = X.on_u4 / X.on_u1\n",
    "\n",
    "    # 用户线上领取次数\n",
    "    temp = online[online.Coupon_id != 0].groupby('User_id').size().reset_index(name='on_u6')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户线上领取率\n",
    "    X['on_u7'] = X.on_u6 / X.on_u1\n",
    "\n",
    "    # 用户线上不消费次数\n",
    "    temp = online[(online.Date == date_null) & (online.Coupon_id != 0)]\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='on_u8')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户线上优惠券核销次数\n",
    "    temp = online[(online.Date != date_null) & (online.Coupon_id != 0)]\n",
    "    temp = temp.groupby('User_id').size().reset_index(name='on_u9')\n",
    "    X = pd.merge(X, temp, how='left', on='User_id')\n",
    "\n",
    "    # 用户线上优惠券核销率\n",
    "    X['on_u10'] = X.on_u9 / X.on_u6\n",
    "\n",
    "    # 用户线下不消费次数占线上线下总的不消费次数的比重\n",
    "    X['on_u11'] = X.u3 / (X.on_u8 + X.u3)\n",
    "\n",
    "    # 用户线下的优惠券核销次数占线上线下总的优惠券核销次数的比重\n",
    "    X['on_u12'] = X.u2 / (X.on_u9 + X.u2)\n",
    "\n",
    "    # 用户线下领取的记录数量占总的记录数量的比重\n",
    "    X['on_u13'] = X.u1 / (X.on_u6 + X.u1)\n",
    "\n",
    "    # # 消费优惠券的平均折率\n",
    "    # temp = coupon_consume.groupby('User_id').discount_rate.mean().reset_index(name='ou14')\n",
    "    # X = pd.merge(X, temp, how='left', on='User_id')\n",
    "    #\n",
    "    # # 用户核销优惠券的最低消费折率\n",
    "    # temp = coupon_consume.groupby('User_id').discount_rate.min().reset_index(name='ou15')\n",
    "    # X = pd.merge(X, temp, how='left', on='User_id')\n",
    "    #\n",
    "    # # 用户核销优惠券的最高消费折率\n",
    "    # temp = coupon_consume.groupby('User_id').discount_rate.max().reset_index(name='ou16')\n",
    "    # X = pd.merge(X, temp, how='left', on='User_id')\n",
    "    #\n",
    "    # # 不同打折优惠券领取次数\n",
    "    # temp = online.groupby('Discount_rate').size().reset_index(name='oc1')\n",
    "    # X = pd.merge(X, temp, how='left', on='Discount_rate')\n",
    "    #\n",
    "    # # 不同打折优惠券使用次数\n",
    "    # temp = coupon_consume.groupby('Discount_rate').size().reset_index(name='oc2')\n",
    "    # X = pd.merge(X, temp, how='left', on='Discount_rate')\n",
    "    #\n",
    "    # # 不同打折优惠券不使用次数\n",
    "    # temp = coupon_no_consume.groupby('Discount_rate').size().reset_index(name='oc3')\n",
    "    # X = pd.merge(X, temp, how='left', on='Discount_rate')\n",
    "    #\n",
    "    # # 不同打折优惠券使用率\n",
    "    # X['oc4'] = X.oc2 / X.oc1\n",
    "\n",
    "    print(len(X), len(X.columns))\n",
    "    print('----------')\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def task(X_chunk, X, counter):\n",
    "    print(counter, end=',', flush=True)\n",
    "    X_chunk = X_chunk.copy()\n",
    "\n",
    "    X_chunk['o17'] = -1\n",
    "    X_chunk['o18'] = -1\n",
    "\n",
    "    for i, user in X_chunk.iterrows():\n",
    "        temp = X[X.User_id == user.User_id]\n",
    "\n",
    "        temp1 = temp[temp.Date_received < user.Date_received]\n",
    "        temp2 = temp[temp.Date_received > user.Date_received]\n",
    "\n",
    "        # 用户此次之后/前领取的所有优惠券数目\n",
    "        X_chunk.loc[i, 'o3'] = len(temp1)\n",
    "        X_chunk.loc[i, 'o4'] = len(temp2)\n",
    "\n",
    "        # 用户此次之后/前领取的特定优惠券数目\n",
    "        X_chunk.loc[i, 'o5'] = len(temp1[temp1.Coupon_id == user.Coupon_id])\n",
    "        X_chunk.loc[i, 'o6'] = len(temp2[temp2.Coupon_id == user.Coupon_id])\n",
    "\n",
    "        # 用户上/下一次领取的时间间隔\n",
    "        temp1 = temp1.sort_values(by='Date_received', ascending=False)\n",
    "        if len(temp1):\n",
    "            X_chunk.loc[i, 'o17'] = (user.Date_received - temp1.iloc[0].Date_received).days\n",
    "\n",
    "        temp2 = temp2.sort_values(by='Date_received')\n",
    "        if len(temp2):\n",
    "            X_chunk.loc[i, 'o18'] = (temp2.iloc[0].Date_received - user.Date_received).days\n",
    "\n",
    "    return X_chunk\n",
    "\n",
    "\n",
    "# In[]\n",
    "if __name__ == '__main__':\n",
    "    # 程序开始时间打印\n",
    "    start = datetime.datetime.now()\n",
    "    print(start.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    cpu_jobs = os.cpu_count() - 1\n",
    "    date_null = pd.to_datetime('1970-01-01', format='%Y-%m-%d')\n",
    "\n",
    "    pd.set_option('expand_frame_repr', False)\n",
    "    pd.set_option('display.max_rows', 200)\n",
    "    pd.set_option('display.max_columns', 200)\n",
    "\n",
    "    # 预处理后数据存放路径\n",
    "    FeaturePath = '../data/data_preprocessed_2'\n",
    "    if not os.path.exists(FeaturePath):\n",
    "        os.mkdir(FeaturePath)\n",
    "    \n",
    "    print('read')\n",
    "    # 读入源数据\n",
    "    off_train, on_train, off_test = get_source_data()\n",
    "    print('null')\n",
    "    # 源数据null处理\n",
    "    off_train = null_process_offline(off_train, predict=False)\n",
    "    on_train = null_process_online(on_train)\n",
    "    off_test = null_process_offline(off_test, predict=True)\n",
    "    print('feature')\n",
    "    # 获取训练特征集，测试特征集\n",
    "    ProcessDataSet1, ProcessDataSet2, ProcessDataSet3 = data_process(off_train, on_train, off_test)\n",
    "    print('save')\n",
    "    # 源数据处理后的数据保存为文件\n",
    "    # dataset_1 = get_offline_features(dataset1, feature1_off)\n",
    "    # ProcessDataSet1 = get_online_features(feature1_on, dataset_1)\n",
    "    ProcessDataSet1.to_csv(os.path.join(FeaturePath, 'ProcessDataSet1.csv'), index=None)\n",
    "\n",
    "    # dataset_2 = get_offline_features(dataset2, feature2_off)\n",
    "    # ProcessDataSet2 = get_online_features(feature2_on, dataset_2)\n",
    "    ProcessDataSet2.to_csv(os.path.join(FeaturePath, 'ProcessDataSet2.csv'), index=None)\n",
    "\n",
    "    # dataset_3 = get_offline_features(dataset3, feature3_off)\n",
    "    # ProcessDataSet3 = get_online_features(feature3_on, dataset_3)\n",
    "    ProcessDataSet3.to_csv(os.path.join(FeaturePath, 'ProcessDataSet3.csv'), index=None)\n",
    "\n",
    "    # 花费时间打印\n",
    "    print((datetime.datetime.now() - start).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9dbd4c2beebe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mceil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from math import ceil\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import log_loss, roc_auc_score, auc, roc_curve\n",
    "\n",
    "\n",
    "def get_processed_data():\n",
    "    dataset1 = pd.read_csv('data_preprocessed_2/ProcessDataSet1.csv')\n",
    "    dataset2 = pd.read_csv('data_preprocessed_2/ProcessDataSet2.csv')\n",
    "    dataset3 = pd.read_csv('data_preprocessed_2/ProcessDataSet3.csv')\n",
    "\n",
    "    dataset1.drop_duplicates(inplace=True)\n",
    "    dataset2.drop_duplicates(inplace=True)\n",
    "    dataset3.drop_duplicates(inplace=True)\n",
    "\n",
    "    dataset12 = pd.concat([dataset1, dataset2], axis=0)\n",
    "\n",
    "    dataset12.fillna(0, inplace=True)\n",
    "    dataset3.fillna(0, inplace=True)\n",
    "\n",
    "    return dataset12, dataset3\n",
    "\n",
    "\n",
    "def train_xgb(dataset12, dataset3):\n",
    "    predict_dataset = dataset3[['User_id', 'Coupon_id', 'Date_received']].copy()\n",
    "    predict_dataset.Date_received = pd.to_datetime(predict_dataset.Date_received, format='%Y-%m-%d')\n",
    "    predict_dataset.Date_received = predict_dataset.Date_received.dt.strftime('%Y%m%d')\n",
    "\n",
    "    # 将数据转化为dmatric格式\n",
    "    dataset12_x = dataset12.drop(\n",
    "        columns=['User_id', 'Merchant_id', 'Discount_rate', 'Date_received', 'discount_rate_x', 'discount_rate_y',\n",
    "                 'Date', 'Coupon_id', 'label'], axis=1)\n",
    "    dataset3_x = dataset3.drop(\n",
    "        columns=['User_id', 'Merchant_id', 'Discount_rate', 'Date_received', 'discount_rate_x', 'discount_rate_y',\n",
    "                 'Coupon_id'], axis=1)\n",
    "\n",
    "    train_dmatrix = xgb.DMatrix(dataset12_x, label=dataset12.label)\n",
    "    predict_dmatrix = xgb.DMatrix(dataset3_x)\n",
    "\n",
    "    # xgboost模型训练\n",
    "    params = {'booster': 'gbtree',\n",
    "              'objective': 'binary:logistic',\n",
    "              'eval_metric': 'auc',\n",
    "              'gamma': 0.1,\n",
    "              'min_child_weight': 1.1,\n",
    "              'max_depth': 5,\n",
    "              'lambda': 10,\n",
    "              'subsample': 0.7,\n",
    "              'colsample_bytree': 0.7,\n",
    "              'colsample_bylevel': 0.7,\n",
    "              'eta': 0.01,\n",
    "            #   'tree_method': 'gpu_hist',\n",
    "            #   'n_gpus': '-1',\n",
    "              'seed': 0,\n",
    "              'nthread': cpu_jobs,\n",
    "            #   'predictor': 'gpu_predictor'\n",
    "              }\n",
    "\n",
    "    # 使用xgb.cv优化num_boost_round参数\n",
    "    cvresult = xgb.cv(params, train_dmatrix, num_boost_round=10000, nfold=2, metrics='auc', seed=0, callbacks=[\n",
    "        xgb.callback.print_evaluation(show_stdv=False),\n",
    "        xgb.callback.early_stop(50)\n",
    "    ])\n",
    "    num_round_best = cvresult.shape[0] - 1\n",
    "    print('Best round num: ', num_round_best)\n",
    "\n",
    "    # 使用优化后的num_boost_round参数训练模型\n",
    "    watchlist = [(train_dmatrix, 'train')]\n",
    "    model = xgb.train(params, train_dmatrix, num_boost_round=num_round_best, evals=watchlist)\n",
    "\n",
    "    model.save_model('train_dir_2/xgbmodel')\n",
    "    params['predictor'] = 'cpu_predictor'\n",
    "    model = xgb.Booster(params)\n",
    "    model.load_model('train_dir_2/xgbmodel')\n",
    "\n",
    "    # predict test set\n",
    "    dataset3_predict = predict_dataset.copy()\n",
    "    dataset3_predict['label'] = model.predict(predict_dmatrix)\n",
    "\n",
    "    # 标签归一化\n",
    "    dataset3_predict.label = MinMaxScaler(copy=True, feature_range=(0, 1)).fit_transform(\n",
    "        dataset3_predict.label.values.reshape(-1, 1))\n",
    "    dataset3_predict.sort_values(by=['Coupon_id', 'label'], inplace=True)\n",
    "    dataset3_predict.to_csv(\"train_dir_2/xgb_preds.csv\", index=None, header=None)\n",
    "    print(dataset3_predict.describe())\n",
    "\n",
    "    # 在dataset12上计算auc\n",
    "    # model = xgb.Booster()\n",
    "    # model.load_model('train_dir_2/xgbmodel')\n",
    "\n",
    "    temp = dataset12[['Coupon_id', 'label']].copy()\n",
    "    temp['pred'] = model.predict(xgb.DMatrix(dataset12_x))\n",
    "    temp.pred = MinMaxScaler(copy=True, feature_range=(0, 1)).fit_transform(temp['pred'].values.reshape(-1, 1))\n",
    "    print(myauc(temp))\n",
    "\n",
    "\n",
    "# 性能评价函数\n",
    "def myauc(test):\n",
    "    testgroup = test.groupby(['Coupon_id'])\n",
    "    aucs = []\n",
    "    for i in testgroup:\n",
    "        tmpdf = i[1]\n",
    "        if len(tmpdf['label'].unique()) != 2:\n",
    "            continue\n",
    "        fpr, tpr, thresholds = roc_curve(tmpdf['label'], tmpdf['pred'], pos_label=1)\n",
    "        aucs.append(auc(fpr, tpr))\n",
    "    return np.average(aucs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = datetime.datetime.now()\n",
    "    print(start.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    # log = '%s\\n' % start.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    cpu_jobs = os.cpu_count() - 1\n",
    "    date_null = pd.to_datetime('1970-01-01', format='%Y-%m-%d')\n",
    "\n",
    "    dataset12, dataset3 = get_processed_data()\n",
    "    # analysis()\n",
    "    # detect_duplicate_columns()\n",
    "    # feature_importance_score()\n",
    "\n",
    "    # grid_search_gbdt()\n",
    "    # train_gbdt()\n",
    "    # predict('gbdt')\n",
    "\n",
    "    # grid_search_xgb()\n",
    "    train_xgb(dataset12, dataset3)\n",
    "\n",
    "    # print('predict: start predicting......')\n",
    "    # # predict('xgb')\n",
    "    # print('predict: predicting finished.')\n",
    "\n",
    "    # log += 'time: %s\\n' % str((datetime.datetime.now() - start)).split('.')[0]\n",
    "    # log += '----------------------------------------------------\\n'\n",
    "    # open('%s.log' % os.path.basename(__file__), 'a').write(log)\n",
    "    # print(log)\n",
    "    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    print('time costed is: %s s' % (datetime.datetime.now() - start).seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_data():\n",
    "    dataset1 = pd.read_csv('../data/data_preprocessed_2/ProcessDataSet1.csv')\n",
    "    dataset2 = pd.read_csv('../data/data_preprocessed_2/ProcessDataSet2.csv')\n",
    "    dataset3 = pd.read_csv('../data/data_preprocessed_2/ProcessDataSet3.csv')\n",
    "\n",
    "    dataset1.drop_duplicates(inplace=True)\n",
    "    dataset2.drop_duplicates(inplace=True)\n",
    "    dataset3.drop_duplicates(inplace=True)\n",
    "\n",
    "    dataset12 = pd.concat([dataset1, dataset2], axis=0)\n",
    "\n",
    "    dataset12.fillna(0, inplace=True)\n",
    "    dataset3.fillna(0, inplace=True)\n",
    "\n",
    "    return dataset12, dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
